{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Lines master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from skimage import img_as_ubyte\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "from scipy.signal import gaussian\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera calibration\n",
    "Here the board is a 9x6 bord. Some pictures are too close to find chess corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of calibration image\n",
    "cal_images = glob.glob('./camera_cal/calibration*.jpg')\n",
    "\n",
    "objpoints = [] #3D points in real world\n",
    "imgpoints = [] #2D points in image space\n",
    "\n",
    "#prepare object points like (0,0,0) (1,0,0)... (9,6,0) for 9x6 grid\n",
    "objpts69 = np.zeros((6*9,3), np.float32)\n",
    "objpts69[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "# Prepare display of images\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "fig.suptitle('Finding chessboard corner')\n",
    "\n",
    "for i, imname in enumerate(cal_images):\n",
    "    #read in each image\n",
    "    img = mpimg.imread(imname)\n",
    "    \n",
    "    #convert image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Finding chessboard corners (for an 9x6 board)\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (9,6), None)\n",
    "    \n",
    "    # If corners are found (for an 9x6 board), add corners and objet points\n",
    "    if ret == True:\n",
    "        imgpoints.append(corners)\n",
    "        objpoints.append(objpts69)\n",
    "    \n",
    "# Calibrate and obtain the coefficients \n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distortion correction\n",
    "Like in explanation. Nothing has been add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cal_undistort(img):\n",
    "     return cv2.undistort(img, mtx, dist, None, mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = mpimg.imread(cal_images[7])\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Original Image', fontsize=50)\n",
    "ax2.imshow(cal_undistort(img))\n",
    "ax2.set_title('Undistorted Image', fontsize=50)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_images = np.sort(glob.glob('./test_images/*.jpg'))\n",
    "\n",
    "undistort_images = []\n",
    "\n",
    "for imname in test_images:\n",
    "    img = mpimg.imread(imname)\n",
    "    undistort_img = cal_undistort(img)\n",
    "    undistort_images.append(undistort_img)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Original Image', fontsize=50)\n",
    "ax2.imshow(undistort_img)\n",
    "ax2.set_title('Undistorted Image', fontsize=50)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color/gradient threshold\n",
    "Lane edge finding by using color space and gradient has been thresholded with the challenge video.\n",
    "My favoritesolution use:\n",
    "* Gradient on V channel of HSV space : to detect easily  white line on dark back. (Better than R in RGB space)\n",
    "* Gradient on S channel of HLS space : to detect especialy yellow line on white or shadow back. (Better than S in HSV space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def abs_thresh(img, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Calculate gradient abs \n",
    "    if orient == 'x':\n",
    "        sobel = cv2.Sobel(img, cv2.CV_64F, 1, 0)\n",
    "    else:\n",
    "        sobel = cv2.Sobel(img, cv2.CV_64F, 0, 1) \n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Apply threshold\n",
    "    grad_binary = np.zeros_like(scaled_sobel)\n",
    "    grad_binary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return grad_binary\n",
    "\n",
    "def mag_thresh(img, sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Calculate gradient magnitude\n",
    "    sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    sobel = np.sqrt(sobelx**2 + sobely**2)\n",
    "    scaled_sobel = np.uint8(255*sobel/np.max(sobel))\n",
    "    # Apply threshold\n",
    "    mag_binary = np.zeros_like(scaled_sobel)\n",
    "    mag_binary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return mag_binary\n",
    "\n",
    "def dir_thresh(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # Calculate gradient direction\n",
    "    sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely) \n",
    "    dir_grad = np.arctan2(abs_sobely, abs_sobelx)\n",
    "    # Apply threshold\n",
    "    dir_binary = np.zeros_like(dir_grad)\n",
    "    dir_binary[(dir_grad >= thresh[0]) & (dir_grad <= thresh[1])] = 1\n",
    "    return dir_binary\n",
    "\n",
    "\n",
    "def gradients_threshold(img, ksize=3, gradx_th=(30, 255), grady_th=(30, 255), mag_th=(30, 255), dir_th=(0.7, 1.3)): \n",
    "\n",
    "    # Apply each of the thresholding functions\n",
    "    gradx_binary = abs_thresh(img, orient='x', sobel_kernel=ksize, thresh=gradx_th)\n",
    "    grady_binary = abs_thresh(img, orient='y', sobel_kernel=ksize, thresh=grady_th)\n",
    "    mag_binary = mag_thresh(img, sobel_kernel=ksize, thresh=mag_th)\n",
    "    dir_binary = dir_thresh(img, sobel_kernel=ksize, thresh=dir_th)\n",
    "    \n",
    "    combined = np.zeros_like(dir_binary)\n",
    "    combined[((gradx_binary == 1) & (grady_binary == 1)) | ((mag_binary == 1) & (dir_binary == 1))] = 1\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def threshold_pipeline(img, s_thresh=(120, 255), h_thresh=(0, 50)):\n",
    "    \n",
    "    img = np.copy(img)\n",
    "    #clahe = equalize_adapthist(img).astype(np.float32)\n",
    "    #clahe = img_as_ubyte(clahe)\n",
    "    \n",
    "    # Convert to HLS and HSV color space and separate the channels\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV).astype(np.float)\n",
    "    v_channel = hsv[:,:,2]\n",
    "    \n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS).astype(np.float)\n",
    "    s_channel = hls[:,:,2]\n",
    "    \n",
    "    # Threshold combined gradient of S channel with more accurate thresholds\n",
    "    grads_binary = gradients_threshold(s_channel, gradx_th=(15, 255), grady_th=(15, 255), mag_th=(25, 255), dir_th=(0.7, 1.3))\n",
    "    \n",
    "    # Threshold combined gradient of V channel\n",
    "    gradv_binary = gradients_threshold(v_channel)\n",
    "    \n",
    "    \n",
    "    combined_binary = np.zeros_like(gradv_binary)\n",
    "    combined_binary[((grads_binary == 1) | (gradv_binary == 1))] = 1    \n",
    "    \n",
    "    return img_as_ubyte(combined_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold_images = []\n",
    "\n",
    "for img in undistort_images:\n",
    "    threshold_img = threshold_pipeline(img)\n",
    "    threshold_images.append(threshold_img)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Undistorted Image', fontsize=50)\n",
    "ax2.imshow(threshold_img, cmap='gray')\n",
    "ax2.set_title('Threshold Image', fontsize=50)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective transform\n",
    "warp your image to a top-down view. Works has been to define a correct destination and source area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_straight1 = np.copy(undistort_images[0])\n",
    "image_straight2 = np.copy(undistort_images[1])\n",
    "image_size = image_straight1.shape[0:2]\n",
    "\n",
    "\n",
    "#pts = np.array([[200,image_size[0]],[1120,image_size[0]],[689,450],[593,450]], np.int32)\n",
    "pts = np.array([[200,image_size[0]],[1120,image_size[0]],[723,470],[563,470]], np.int32)\n",
    "\n",
    "cv2.polylines(image_straight1,[pts],True,(255,0,0), thickness=2)\n",
    "cv2.polylines(image_straight2,[pts],True,(255,0,0), thickness=2)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(image_straight1)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(image_straight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lane width\n",
    "lane_width = 600\n",
    "border = (image_size[1] - lane_width)//2\n",
    "\n",
    "def perspective_transform(img):\n",
    "    img_size = img.shape[:2]\n",
    "    src = np.float32([[200,image_size[0]],[1120,image_size[0]],[723,470],[563,470]])\n",
    "    dst = np.float32([[border, image_size[0]],[border + lane_width, image_size[0]], [border + lane_width, 0],[border, 0]])\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    # use cv2.warpPerspective() to warp your image to a top-down view\n",
    "    warped = cv2.warpPerspective(img, M, image_size[1::-1], flags=cv2.INTER_LINEAR)    \n",
    "    # return warped and Matrix for later when we will inverse the transformation\n",
    "    return M, warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_ptransformed1 = np.copy(undistort_images[0])\n",
    "image_ptransformed2 = np.copy(undistort_images[1])\n",
    "\n",
    "_,image_ptransformed1 = perspective_transform(image_ptransformed1)\n",
    "_,image_ptransformed2 = perspective_transform(image_ptransformed2)\n",
    "\n",
    "img_size = image_ptransformed1.shape[:2]\n",
    "image_ptransformed1 = cv2.rectangle(image_ptransformed1, (border, 0), (border + lane_width, img_size[0]), (255,0,0), thickness=2)\n",
    "image_ptransformed2 = cv2.rectangle(image_ptransformed2, (border, 0), (border + lane_width, img_size[0]), (255,0,0), thickness=2)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(image_straight1)\n",
    "ax1.set_title('Undistorted Image', fontsize=50)\n",
    "ax2.imshow(image_ptransformed1, cmap='gray')\n",
    "ax2.set_title('Perspective transformed Image', fontsize=50)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(image_straight2)\n",
    "ax1.set_title('Undistorted Image', fontsize=50)\n",
    "ax2.imshow(image_ptransformed2, cmap='gray')\n",
    "ax2.set_title('Perspective transformed Image', fontsize=50)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perspective_images = []\n",
    "\n",
    "for img in threshold_images:\n",
    "    _,perspective_img = perspective_transform(img)\n",
    "    perspective_images.append(perspective_img)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(img, cmap='gray')\n",
    "ax1.set_title('Threshold Image', fontsize=50)\n",
    "ax2.imshow(perspective_img, cmap='gray')\n",
    "ax2.set_title('Perspective transformed Image', fontsize=50)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking\n",
    "class to receive the characteristics of each line detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a class to receive the characteristics of each line detection\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None     \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])]  \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "        #x values for detected line pixels\n",
    "        self.allx = None  \n",
    "        #y values for detected line pixels\n",
    "        self.ally = None\n",
    "        # image iteration\n",
    "        self.iteration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Detect lane lines\n",
    "   * If the first image or if last detection have less than 2000 pixels or each 10 images:\n",
    "       * Define a gaussian mask in order to favour line in the center during the bottom line detection center\n",
    "       * Define a gaussian window for line detection. Normal distribution is better to center it.\n",
    "       * By convolution, find the optimal fit\n",
    "   * else, detect line from the last fits with a margin of 60 pixels\n",
    "   * Smooth the fit with the 10th last fit with a learning rate of 0.9, then average with a ratio of 70%/30% between left and right lines.\n",
    "   * Display and select, pixel inside the windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To detect lane more accurately at the bottom of image, we define a mask which focus on the center of image\n",
    "\n",
    "focus_window = 2*gaussian(1280, std=300)\n",
    "plt.plot(focus_window)\n",
    "plt.title(\"Mask for center focusing\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.xlabel(\"pixel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# function for detection who prefer the center of image. Used only for choosing the bottom of line \n",
    "def center_focus(img, lane_width=lane_width):\n",
    "    focus_window = 2 * gaussian(img.shape[1], std=200) + 1\n",
    "    new_img = []\n",
    "    for row in img:\n",
    "        new_img.append(row * focus_window)\n",
    "    new_img = np.array(new_img)\n",
    "    return new_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convolution window : detect lines with normal distribution to accurately center of line\n",
    "window = gaussian(151, std=20)\n",
    "\n",
    "plt.plot(window)\n",
    "plt.title(\"Window for convolution\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.xlabel(\"pixel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# selection window settings\n",
    "window_width = 50 \n",
    "window_height = 20 # Break image into 9 vertical layers since image height is 720\n",
    "margin = 40 # How much to slide left and right for searching\n",
    "\n",
    "def window_mask(width, height, img_ref, center,level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),\n",
    "           max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "    return output\n",
    "\n",
    "def find_window_centroids(warped, window_width=window_width, window_height=window_height, margin=margin):\n",
    "    \n",
    "    # In order to maximize detection lane in center\n",
    "    focus_warped = center_focus(warped)\n",
    "    \n",
    "    window_centroids = [] # Store the (left,right) window centroid positions per level\n",
    "    # Create our window template that we will use for convolutions\n",
    "    window = gaussian(151, std=20)\n",
    "    \n",
    "    # First find the two starting positions for the left and right lane by using np.sum to get the vertical image slice\n",
    "    # and then np.convolve the vertical image slice with the window template \n",
    "    \n",
    "    # Use len(window)/2 as offset because convolution signal reference is at right side of window, not center of window\n",
    "    offset = len(window)/2\n",
    "    \n",
    "    # Sum half bottom of image to get slice\n",
    "    l_sum = np.sum(focus_warped[int(focus_warped.shape[0]/2):,:int(focus_warped.shape[1]/2)], axis=0)\n",
    "    l_center = np.argmax(np.convolve(window,l_sum))-offset\n",
    "    r_sum = np.sum(focus_warped[int(focus_warped.shape[0]/2):,int(focus_warped.shape[1]/2):], axis=0)\n",
    "    r_center = np.argmax(np.convolve(window,r_sum))-offset+int(focus_warped.shape[1]/2)\n",
    "    \n",
    "    # Add what we found for the first layer\n",
    "    window_centroids.append((l_center,r_center))\n",
    "    \n",
    "    # Go through each layer looking for max pixel locations\n",
    "    for level in range(1,(int)(warped.shape[0]/window_height)):\n",
    "        # convolve the window into the vertical slice of the image\n",
    "        image_layer = np.sum(warped[int(warped.shape[0]-(level+1)*window_height):int(warped.shape[0]-level*window_height),:], axis=0)\n",
    "        conv_signal = np.convolve(window, image_layer)\n",
    "        # Find the best left centroid by using past left center as a reference\n",
    "        l_min_index = int(max(l_center+offset-margin,0))\n",
    "        l_max_index = int(min(l_center+offset+margin,warped.shape[1]))\n",
    "        if np.max(conv_signal[l_min_index:l_max_index]) > 10000.: #if more about 50 pixels\n",
    "            l_center = np.argmax(conv_signal[l_min_index:l_max_index])+l_min_index-offset\n",
    "        # Find the best right centroid by using past right center as a reference\n",
    "        r_min_index = int(max(r_center+offset-margin,0))\n",
    "        r_max_index = int(min(r_center+offset+margin,warped.shape[1]))\n",
    "        if np.max(conv_signal[r_min_index:r_max_index]) > 10000.: #if more about 50 pixels\n",
    "            r_center = np.argmax(conv_signal[r_min_index:r_max_index])+r_min_index-offset\n",
    "        # Add what we found for that layer\n",
    "        window_centroids.append((l_center,r_center))\n",
    "\n",
    "    return window_centroids\n",
    "\n",
    "def display_window(warped):\n",
    "    window_centroids = find_window_centroids(warped)\n",
    "\n",
    "    # If we found any window centers\n",
    "    if len(window_centroids) > 0:\n",
    "\n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(warped)\n",
    "        r_points = np.zeros_like(warped)\n",
    "\n",
    "        # Go through each level and draw the windows \t\n",
    "        for level in range(0,len(window_centroids)):\n",
    "            # Window_mask is a function to draw window areas\n",
    "            l_mask = window_mask(window_width,window_height,warped,window_centroids[level][0],level)\n",
    "            r_mask = window_mask(window_width,window_height,warped,window_centroids[level][1],level)\n",
    "            # Add graphic points from window mask here to total pixels found \n",
    "            l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "            r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "\n",
    "            # Draw the results\n",
    "            template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "            zero_channel = np.zeros_like(template) # create a zero color channle \n",
    "            template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window pixels green\n",
    "            warpage = np.array(cv2.merge((warped,warped,warped)),np.uint8) # making the original road pixels 3 color channels\n",
    "            output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    "\n",
    "    # If no window centers found, just display orginal road image\n",
    "    else:\n",
    "        output = np.array(cv2.merge((warped,warped,warped)),np.uint8)\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for img in perspective_images:\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    img1 = display_window(img)\n",
    "    ax1.imshow(img, cmap='gray')\n",
    "    ax1.set_title('Perspective transformed', fontsize=50)\n",
    "    ax2.imshow(img1)\n",
    "    ax2.set_title('window fitting results', fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_lines(binary_warped, left, right, display=False, n=10):\n",
    "    \n",
    "    # Create an output image to draw on and  visualize the result\n",
    "        \n",
    "    ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    \n",
    "    # Create empty lists to receive left and right lane pixels  by windows\n",
    "    left_lane = []\n",
    "    right_lane = []\n",
    "    \n",
    "    if left.detected == False and right.detected == False:\n",
    "        window_centroids = find_window_centroids(binary_warped)\n",
    "        \n",
    "        if display == True:\n",
    "            out_img = display_window(binary_warped)\n",
    "        \n",
    "        # Go through each level and draw the windows\n",
    "        for level in range(0,len(window_centroids)):\n",
    "            # Window_mask is a function to draw window areas\n",
    "            l_mask = window_mask(window_width,window_height,binary_warped,window_centroids[level][0],level)\n",
    "            r_mask = window_mask(window_width,window_height,binary_warped,window_centroids[level][1],level)\n",
    "            # Identify the nonzero pixels in x and y within the window \n",
    "            l_points = l_mask * binary_warped\n",
    "            r_points = r_mask * binary_warped\n",
    "\n",
    "            # Append these points to the lists\n",
    "            left_lane.append(l_points)\n",
    "            right_lane.append(r_points)\n",
    "    \n",
    "        # Sum the arrays of pixels by window to obtain the whole pixels\n",
    "        left_lane_all = np.sum(np.array(left_lane), axis=0)\n",
    "        right_lane_all = np.sum(np.array(right_lane), axis=0)\n",
    "        \n",
    "        # Extract left and right line pixel positions\n",
    "        left.allx = left_lane_all.nonzero()[1]\n",
    "        left.ally = left_lane_all.nonzero()[0] \n",
    "        right.allx = right_lane_all.nonzero()[1]\n",
    "        right.ally = right_lane_all.nonzero()[0]\n",
    "        \n",
    "    else:\n",
    "        # Assume you now have a new warped binary image \n",
    "        # from the next frame of video (also called \"binary_warped\")\n",
    "        # It's now much easier to find line pixels!\n",
    "        nonzero = binary_warped.nonzero()\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        find_margin = 60 #width of detection area\n",
    "        left_lane_inds = ((nonzerox > (left.best_fit[0]*(nonzeroy**2) + left.best_fit[1]*nonzeroy + left.best_fit[2] - find_margin)) \n",
    "                        & (nonzerox < (left.best_fit[0]*(nonzeroy**2) + left.best_fit[1]*nonzeroy + left.best_fit[2] + find_margin))) \n",
    "        right_lane_inds = ((nonzerox > (right.best_fit[0]*(nonzeroy**2) + right.best_fit[1]*nonzeroy + right.best_fit[2] - find_margin)) \n",
    "                        & (nonzerox < (right.best_fit[0]*(nonzeroy**2) + right.best_fit[1]*nonzeroy + right.best_fit[2] + find_margin)))  \n",
    "\n",
    "    \n",
    "        # Again, extract left and right line pixel positions\n",
    "        left.allx = nonzerox[left_lane_inds]\n",
    "        left.ally = nonzeroy[left_lane_inds] \n",
    "        right.allx = nonzerox[right_lane_inds]\n",
    "        right.ally = nonzeroy[right_lane_inds]\n",
    "    \n",
    "        if display == True:\n",
    "            out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "            window_img = np.zeros_like(out_img)\n",
    "            # Generate a polygon to illustrate the search window area\n",
    "            # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "            left_line_window1 = np.array([np.transpose(np.vstack([left.bestx-find_margin, ploty]))])\n",
    "            left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left.bestx+find_margin, ploty])))])\n",
    "            left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "            right_line_window1 = np.array([np.transpose(np.vstack([right.bestx-find_margin, ploty]))])\n",
    "            right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right.bestx+find_margin, ploty])))])\n",
    "            right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "            # Draw the lane onto the warped blank image\n",
    "            cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "            cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "            out_img = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "    \n",
    "    \n",
    "    if display == True:\n",
    "        out_img[left.ally, left.allx] = [255, 0, 0]\n",
    "        out_img[right.ally, right.allx] = [0, 0, 255]\n",
    "    \n",
    "    # Fit a second order polynomial to each\n",
    "    if len(left.ally) > 0:\n",
    "        left.current_fit = np.polyfit(left.ally, left.allx, 2)\n",
    "    if len(right.ally) > 0:\n",
    "        right.current_fit = np.polyfit(right.ally, right.allx, 2)\n",
    "    \n",
    "    # the last polynomial fit\n",
    "    left.recent_xfitted.append(left.current_fit[0]*ploty**2 + left.current_fit[1]*ploty + left.current_fit[2])\n",
    "    left.recent_xfitted = left.recent_xfitted[-n:]\n",
    "    right.recent_xfitted.append(right.current_fit[0]*ploty**2 + right.current_fit[1]*ploty + right.current_fit[2])\n",
    "    right.recent_xfitted = right.recent_xfitted[-n:]\n",
    "    \n",
    "    # average with a learning rate of 0.9\n",
    "    left.bestx = np.average(np.array(left.recent_xfitted), axis=0, weights=[0.9**i for i in range(len(left.recent_xfitted))])\n",
    "    right.bestx = np.average(np.array(right.recent_xfitted), axis=0, weights=[0.9**i for i in range(len(right.recent_xfitted))])\n",
    "    # average with ration 70%/30% between left and right lines\n",
    "    left.bestx = np.average([left_line.bestx, right_line.bestx + left_line.bestx[-1] - right_line.bestx[-1]],\n",
    "                            axis=0, weights=[0.7, 0.3])\n",
    "    right.bestx = np.average([right.bestx, left_line.bestx + right_line.bestx[-1] - left_line.bestx[-1]], \n",
    "                            axis=0, weights=[0.7, 0.3])\n",
    "    \n",
    "    left.best_fit = np.polyfit(ploty, left.bestx, 2)\n",
    "    right.best_fit = np.polyfit(ploty, right.bestx, 2)\n",
    "    \n",
    "    # image iteration in the video\n",
    "    left.iteration += 1\n",
    "    right.iteration += 1\n",
    "    \n",
    "    # if there are less than 2000 fit pixels, Use convolution find policy\n",
    "    if len(left.allx) < 2000 or len(right.allx) < 2000: \n",
    "        left.detected = False\n",
    "        right.detected = False\n",
    "    # else use the last fit polynom window to find pixels\n",
    "    else:\n",
    "        left.detected = True\n",
    "        right.detected = True\n",
    "    \n",
    "    if display == True:\n",
    "        return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for img in perspective_images:\n",
    "    left_line = Line()\n",
    "    right_line = Line()\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    img1 = find_lines(img, left_line, right_line, display=True)\n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title('Finding windows', fontsize=50)\n",
    "    img2 = find_lines(img, left_line, right_line, display=True)\n",
    "    ax2.imshow(img2)\n",
    "    ploty = np.linspace(0, img.shape[0]-1, img.shape[0] )\n",
    "    ax2.plot(left_line.bestx, ploty, color='yellow')\n",
    "    ax2.plot(right_line.bestx, ploty, color='yellow')\n",
    "    ax2.set_title('Finding Lines', fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the lane curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lane_curvature(img_ref, left, right):\n",
    "    \n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 22/img_ref.shape[0] # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/lane_width # meters per pixel in x dimension\n",
    "    \n",
    "    y_eval = float(len(img_ref)-1)\n",
    "    A_left = left.best_fit[0]*xm_per_pix/(ym_per_pix**2)\n",
    "    B_left = left.best_fit[1]*xm_per_pix/ym_per_pix\n",
    "    A_right = right.best_fit[0]*xm_per_pix/(ym_per_pix**2)\n",
    "    B_right = right.best_fit[1]*xm_per_pix/ym_per_pix\n",
    "    \n",
    "    left.radius_of_curvature = ((1 + (2*A_left*y_eval*ym_per_pix + B_left)**2)**1.5) / np.absolute(2*A_left)\n",
    "    right.radius_of_curvature = ((1 + (2*A_right*y_eval*ym_per_pix + B_right)**2)**1.5) / np.absolute(2*A_right)\n",
    "    # Now our radius of curvature is in meters\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find offset of the lane center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def offset(img_ref, left, right):\n",
    "    # Define offset between vehicule center and lane center.  \n",
    "    \n",
    "    # Define conversions in x from pixels space to meters\n",
    "    xm_per_pix = 3.7/lane_width # meters per pixel in x dimension\n",
    "    \n",
    "    y_eval = float(len(img_ref) - 1)\n",
    "    \n",
    "    x_bottom_left = left.best_fit[0]*((y_eval)**2) + left.best_fit[1]*y_eval + left.best_fit[2]\n",
    "    x_bottom_right = right.best_fit[0]*((y_eval)**2) + right.best_fit[1]*y_eval + right.best_fit[2]\n",
    "    \n",
    "    img_center = img_ref.shape[1]/2\n",
    "    \n",
    "    left.line_base_pos = (img_center - x_bottom_left)*xm_per_pix\n",
    "    right.line_base_pos = (img_center - x_bottom_right)*xm_per_pix\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing lane\n",
    "the final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drawing_pipeline(image, left, right):\n",
    "\n",
    "    # undistortion\n",
    "    undist = cal_undistort(image)\n",
    "    # thresholding\n",
    "    thresh = threshold_pipeline(undist)\n",
    "    # Warped image\n",
    "    M, warped = perspective_transform(thresh)\n",
    "    # Detect Lines\n",
    "    find_lines(warped, left, right)\n",
    "    if left.iteration % 10 == 1: # To make more readable values. Update it every 10 pictures\n",
    "        lane_curvature(warped, left, right)\n",
    "        offset(warped, left, right)\n",
    "    radius = np.mean([left.radius_of_curvature, right.radius_of_curvature])\n",
    "    offsetm = (left.line_base_pos + right.line_base_pos)/2\n",
    "    \n",
    "    # Add on images Radius and offset impormation\n",
    "    curvature_string = 'Radius of curvature of lane : {0:.0f}m'.format(radius)\n",
    "    lane_center_offset_string = 'Offset with lane center: {0:.2f}m'.format(offsetm)\n",
    "    \n",
    "    cv2.putText(img=undist, text=curvature_string, org=(350,100), \n",
    "                fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255,255,255), thickness=2)\n",
    "    cv2.putText(img=undist, text=lane_center_offset_string, org=(350,150), \n",
    "                fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255,255,255), thickness=2)\n",
    "    \n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "    \n",
    "    ploty = np.linspace(0, warped.shape[0]-1, warped.shape[0] )\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left.bestx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right.bestx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    Minv = np.linalg.inv(M)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (image.shape[1], image.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(undist, 1, newwarp, 0.3, 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for imname in test_images:\n",
    "    img = mpimg.imread(imname)\n",
    "    left_line = Line()\n",
    "    right_line = Line()\n",
    "    drawing_img = drawing_pipeline(img, left_line, right_line)\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Original Image', fontsize=50)\n",
    "    ax2.imshow(drawing_img)\n",
    "    ax2.set_title('Drawing Lane Image', fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos\n",
    "Function for Building the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_line_pipeline(image, left, right):\n",
    "    undist = cal_undistort(image)\n",
    "    thresh = threshold_pipeline(undist)\n",
    "    M, warped = perspective_transform(thresh)\n",
    "    result = find_lines(warped, left, right, display=True)\n",
    "    return result\n",
    "\n",
    "def find_line_video(clip, left_line, right_line):\n",
    "    def image_find_line (image):\n",
    "        return find_line_pipeline(image, left_line, right_line)\n",
    "    return clip.fl_image(image_find_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tranform any clip with parameters:\n",
    "\n",
    "def final_video(clip, left_line, right_line):\n",
    "    def image_video (image):\n",
    "        return drawing_pipeline(image, left_line, right_line)\n",
    "    return clip.fl_image(image_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video1 : Project Video\n",
    "* video of lane on top-down view warped\n",
    "* Output video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_line = Line()\n",
    "right_line = Line()\n",
    "\n",
    "find_line_output = './output_images/find_line_video.mp4'\n",
    "clip = VideoFileClip('project_video.mp4')\n",
    "find_line_clip = clip.fx(find_line_video, left_line, right_line)\n",
    "%time find_line_clip.write_videofile(find_line_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(find_line_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_line = Line()\n",
    "right_line = Line()\n",
    "\n",
    "output = './output_images/output_video.mp4'\n",
    "clip = VideoFileClip('project_video.mp4')\n",
    "project_clip = clip.fx(final_video, left_line, right_line)\n",
    "%time project_clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 2 : Challenge Video\n",
    "* video of lane on top-down view warped\n",
    "* Output video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_line = Line()\n",
    "right_line = Line()\n",
    "\n",
    "ch_find_line_output = './output_images/find_line_challenge_video.mp4'\n",
    "clip = VideoFileClip('challenge_video.mp4')\n",
    "ch_find_line_clip = clip.fx(find_line_video, left_line, right_line)\n",
    "%time ch_find_line_clip.write_videofile(ch_find_line_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(ch_find_line_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_line = Line()\n",
    "right_line = Line()\n",
    "\n",
    "challenge_output = './output_images/output_challenge_video.mp4'\n",
    "clip = VideoFileClip('challenge_video.mp4')\n",
    "challenge_clip = clip.fx(final_video, left_line, right_line)\n",
    "%time challenge_clip.write_videofile(challenge_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 3: Harder Challenge Video\n",
    "* video of lane on top-down view warped\n",
    "* Output video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_line = Line()\n",
    "right_line = Line()\n",
    "\n",
    "hch_find_line_output = './output_images/find_line_hardchallenge_video.mp4'\n",
    "clip = VideoFileClip('harder_challenge_video.mp4')\n",
    "hch_find_line_clip = clip.fx(find_line_video, left_line, right_line)\n",
    "%time hch_find_line_clip.write_videofile(hch_find_line_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(hch_find_line_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_line = Line()\n",
    "right_line = Line()\n",
    "\n",
    "harder_challenge_output = './output_images/output_harder_challenge_video.mp4'\n",
    "clip = VideoFileClip('harder_challenge_video.mp4')\n",
    "harder_challenge_clip = clip.fx(final_video, left_line, right_line)\n",
    "%time harder_challenge_clip.write_videofile(harder_challenge_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(harder_challenge_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
